{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f3b84d8",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-01 16:49:43.572031: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-01 16:49:46.380863: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-12-01 16:49:46.515216: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-01 16:49:46.515573: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-12-01 16:49:46.828790: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-12-01 16:49:49.280639: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-01 16:49:49.280815: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-01 16:49:49.280836: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "# from tensorflow.keras import layers\n",
    "# import numpy as np\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22257995",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('data/sample_text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c0ccf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.drop(columns='Unnamed: 0', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70870e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.corpus import stopwords \n",
    "# import string\n",
    "# from nltk.stem.wordnet import WordNetLemmatizer\n",
    "# from nltk import word_tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da4f70e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>australian shepherd  homeabout usavailable pup...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>index of   \\tname\\tlast modified\\tsize\\tdescri...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>menuhomeloan applicationcontact usfaqsterms of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>skip to contentpay with bitcoin25  discount fo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>skip to content 61 3 9028 2716world wide shipp...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  australian shepherd  homeabout usavailable pup...       1\n",
       "1  index of   \\tname\\tlast modified\\tsize\\tdescri...       1\n",
       "2  menuhomeloan applicationcontact usfaqsterms of...       1\n",
       "3  skip to contentpay with bitcoin25  discount fo...       1\n",
       "4  skip to content 61 3 9028 2716world wide shipp...       1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9788ad84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean(text):\n",
    "#     for punctuation in string.punctuation:\n",
    "#         text = text.replace(punctuation, ' ') # Remove Punctuation\n",
    "#     lowercased = text.lower() # Lower Case\n",
    "#     #tokenized = word_tokenize(lowercased) # Tokenize\n",
    "#     #words_only = [word for word in tokenized if word.isalpha()] # Remove numbers\n",
    "#     #stop_words = set(stopwords.words('english')) # Make stopword list\n",
    "#     #without_stopwords = [word for word in words_only if not word in stop_words] # Remove Stop Words\n",
    "#     # lemma=WordNetLemmatizer() # Initiate Lemmatizer\n",
    "#     # lemmatized = [lemma.lemmatize(word) for word in without_stopwords] # Lemmatize\n",
    "#     return lowercased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ed63206",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['clean_text'] = df['text'].apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2968f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['clean_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1cfde60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "# import tensorflow_hub as hub\n",
    "# !pip install tensorflow-text\n",
    "# import tensorflow_text as text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6ec9cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['target']=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adfce6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = df['text']\n",
    "# y = df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8f35eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f81769f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow.keras.preprocessing.text import text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb00a5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "151d97bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Word2Vec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#from gensim.models import Word2Vec\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m word2vec \u001b[38;5;241m=\u001b[39m \u001b[43mWord2Vec\u001b[49m(sentences\u001b[38;5;241m=\u001b[39mX_train, vector_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m60\u001b[39m, min_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, window\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Word2Vec' is not defined"
     ]
    }
   ],
   "source": [
    "#from gensim.models import Word2Vec\n",
    "\n",
    "#word2vec = Word2Vec(sentences=X_train, vector_size=60, min_count=10, window=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c093cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fa01b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# import numpy as np\n",
    "\n",
    "# # Function to convert a sentence (list of words) into a matrix representing the words in the embedding space\n",
    "# def embed_sentence(word2vec, sentence):\n",
    "#     embedded_sentence = []\n",
    "#     for word in sentence:\n",
    "#         if word in word2vec.wv:\n",
    "#             embedded_sentence.append(word2vec.wv[word])\n",
    "        \n",
    "#     return np.array(embedded_sentence)\n",
    "\n",
    "# # Function that converts a list of sentences into a list of matrices\n",
    "# def embedding(word2vec, sentences):\n",
    "#     embed = []\n",
    "    \n",
    "#     for sentence in sentences:\n",
    "#         embedded_sentence = embed_sentence(word2vec, sentence)\n",
    "#         embed.append(embedded_sentence)\n",
    "        \n",
    "#     return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7bdb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed_sentence(word2vec, sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27598f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Embed the training and test sentences\n",
    "# X_train_embed = embedding(word2vec, X_train)\n",
    "# X_test_embed = embedding(word2vec, X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf792a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pad the training and test embedded sentences\n",
    "# X_train_pad = pad_sequences(X_train_embed, dtype='float32', padding='post', maxlen=200)\n",
    "# X_test_pad = pad_sequences(X_test_embed, dtype='float32', padding='post', maxlen=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f58b7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras import Sequential\n",
    "# from tensorflow.keras import layers\n",
    "\n",
    "# def init_model():\n",
    "#     model = Sequential()\n",
    "#     model.add(layers.Masking())\n",
    "#     model.add(layers.LSTM(20, activation='tanh'))\n",
    "#     model.add(layers.Dense(15, activation='relu'))\n",
    "#     model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "#     model.compile(loss='binary_crossentropy',\n",
    "#                   optimizer='rmsprop',\n",
    "#                   metrics=['accuracy'])\n",
    "    \n",
    "#     return model\n",
    "\n",
    "# model = init_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6336f0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# es = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "# model.fit(X_train_pad, y_train, \n",
    "#           batch_size = 32,\n",
    "#           epochs=100,\n",
    "#           validation_split=0.3,\n",
    "#           callbacks=[es]\n",
    "#          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd85c44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Evaluate model \n",
    "\n",
    "#res = model.evaluate(X_test_pad, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db303c62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cd12ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('shims')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "477dee08920871f365696aa3595cd2bf97525b8ffd65a32d7d368e76ff91272e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
