{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3346f433-04e7-4550-aa04-37cfbd9d936b",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af4b637c-e6b3-49f9-b0dd-83a95813b80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-07 09:29:26.070741: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-07 09:29:26.070759: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imread\n",
    "\n",
    "import tensorflow.image as tf_image\n",
    "from tensorflow.keras import layers, models, callbacks, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd7730bd-880e-4d70-a73c-d7d3e10e3e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/lscr/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "import string\n",
    "from nltk import word_tokenize \n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "# from sklearn.model_selection import train_test_split\n",
    "import gensim.downloader as api\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f2bd3b-c0c4-4ec7-a5c4-4f35b8d00364",
   "metadata": {},
   "source": [
    "# Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "382f87c8-b317-42ab-b1de-21338d7975a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "\n",
    "rel_imagedir_path = '../../../data/screenshots/'\n",
    "legit_image_path = os.path.join(rel_imagedir_path, 'legit_screenshots')\n",
    "scam_image_path = os.path.join(rel_imagedir_path, 'scam_screenshots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dec97193-5158-42d3-a573-c7be2a7d6ab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../../../data/screenshots/legit_screenshots',\n",
       " '../../../data/screenshots/scam_screenshots')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "legit_image_path, scam_image_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e682583-386b-4add-ac33-0ae9cd1e784c",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d49d1371-f1a5-498d-936f-c1d7234ead13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(key_df, legit_imagedir_path, scam_imagedir_path):\n",
    "    \n",
    "    X_image = []\n",
    "    X_text = []\n",
    "    y = []\n",
    "    \n",
    "    for index, row in key_df.iterrows():\n",
    "        if index % 500 == 0:\n",
    "            print(f'loaded {index} data points')\n",
    "        \n",
    "        # Append image to X_image\n",
    "        if row['target'] == 0:\n",
    "            image_path = os.path.join(legit_imagedir_path, f\"{row['num_of_picture']}.png\")\n",
    "            np_legit_image = imread(image_path)[:, :, :3] # slice off the alpha channel\n",
    "            np_legit_image = tf_image.resize(np_legit_image, (300, 400))\n",
    "            X_image.append(np_legit_image)\n",
    "            \n",
    "        elif row['target'] == 1:\n",
    "            image_path = os.path.join(scam_imagedir_path, f\"{row['num_of_picture']}.png\")\n",
    "            np_legit_image = imread(image_path)[:, :, :3] # slice off the alpha channel\n",
    "            np_legit_image = tf_image.resize(np_legit_image, (300, 400))\n",
    "            X_image.append(np_legit_image)\n",
    "        \n",
    "        # Load text\n",
    "        X_text.append(row['text'])\n",
    "        \n",
    "        y.append(row['target'])\n",
    "    \n",
    "    print('\\nFinished loading data!')\n",
    "    # print(X_image)\n",
    "    return np.array(X_image), X_text, np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79b5bb36-e915-4e9e-a8bd-bc2709ff05d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_key_df = pd.read_csv('merged_key_df.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d89a87e1-031a-422e-a60f-169c524bce11",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-07 09:29:28.101289: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-12-07 09:29:28.101303: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-12-07 09:29:28.101318: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (lscr): /proc/driver/nvidia/version does not exist\n",
      "2022-12-07 09:29:28.101782: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 0 data points\n",
      "loaded 500 data points\n",
      "loaded 1000 data points\n",
      "loaded 1500 data points\n",
      "loaded 2000 data points\n",
      "loaded 2500 data points\n",
      "loaded 3000 data points\n",
      "loaded 3500 data points\n",
      "loaded 4000 data points\n",
      "\n",
      "Finished loading data!\n"
     ]
    }
   ],
   "source": [
    "X_image, X_text, y = load_data(merged_key_df, legit_image_path, scam_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c74063d0-af2a-427e-a512-5591afc42e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "del merged_key_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212d8d44-f75c-4dc3-b356-a6cb0aaf1134",
   "metadata": {},
   "source": [
    "# Proproccess Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a10d88a-13c2-4fe5-bf44-52faa511fd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "def clean(text):\n",
    "    text = text.split()\n",
    "    words_only = [word for word in text if word.isalpha()]\n",
    "    for punctuation in string.punctuation:\n",
    "        words_only = [word.replace(punctuation, ' ').lower() for word in words_only] # Remove Punctuation\n",
    "    # lowercased = text.lower() # Lower Case\n",
    "    #tokenized = word_tokenize(lowercased) # Tokenize\n",
    "    # words_only = [word for word in tokenized if word.isalpha()] # Remove numbers\n",
    "    stop_words = set(stopwords.words('english')) # Make stopword list\n",
    "    without_stopwords = [word for word in words_only if not word in stop_words] # Remove Stop Words\n",
    "    # lemma=WordNetLemmatizer() # Initiate Lemmatizer\n",
    "    # lemmatized = [lemma.lemmatize(word) for word in without_stopwords] # Lemmatize\n",
    "    return without_stopwords\n",
    "\n",
    "def text_cleaner(list_text):\n",
    "    print('Cleaning text')\n",
    "    list_clean_text=[]\n",
    "    for text in list_text:\n",
    "        clean_txt=clean(text) # Use clean function\n",
    "        list_clean_text.append(clean_txt)\n",
    "\n",
    "    return list_clean_text\n",
    "\n",
    "def embed_sentence_with_TF(word2vec, sentence):\n",
    "    embedded_sentence = []\n",
    "    for word in sentence:\n",
    "        if word in word2vec:\n",
    "            embedded_sentence.append(word2vec[word])\n",
    "        \n",
    "    return embedded_sentence\n",
    "\n",
    "# Function that converts a list of sentences into a list of matrices\n",
    "def embedding(word2vec, sentences):\n",
    "    print('Embedding with word2vec')\n",
    "    embed = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        embedded_sentence = embed_sentence_with_TF(word2vec, sentence)\n",
    "        embed.append(embedded_sentence)\n",
    "        \n",
    "    return embed\n",
    "\n",
    "def preprocess_pad(list_text):\n",
    "    # Clean - remove punctuation and stopwords\n",
    "    list_ = text_cleaner(list_text)\n",
    "    \n",
    "    # Embed with word2vec\n",
    "    word2vec_transfer = api.load('glove-wiki-gigaword-100')\n",
    "    list_ = embedding(word2vec_transfer, list_)\n",
    "    \n",
    "    # Pad Sequences\n",
    "    print('Padding sequences')\n",
    "    list_ = pad_sequences(list_, dtype='float32', padding='post', maxlen=200)\n",
    "    \n",
    "    print('Text preprocessing and embedding complete!')\n",
    "    return list_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "658cfb20-b12a-461d-81ad-12154aa3788b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_pad(list_text):\n",
    "#     df_clean=text_cleaner(X_text)\n",
    "#     df_clean.reset_index(drop = True, inplace=True)\n",
    "#     word2vec_transfer = api.load('glove-wiki-gigaword-100')\n",
    "#     X_train_embed_2 = embedding(word2vec_transfer, df_clean['clean_text'])\n",
    "#     X_train_pad = pad_sequences(X_train_embed_2, dtype='float32', padding='post', maxlen=200)\n",
    "#     return X_train_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2957a453-3287-4a59-92a3-5440f951dfb3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning text\n"
     ]
    }
   ],
   "source": [
    "X_text = preprocess_pad(X_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74a9902-113f-4886-9ad6-84482e9d2005",
   "metadata": {},
   "source": [
    "# Functional Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f27d34-2f01-4b2c-87f0-f5eadfee312f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    \n",
    "    # CNN Architecture\n",
    "    cnn_input = layers.Input(shape=X_image.shape[1:])\n",
    "\n",
    "    x = layers.Conv2D(16, (4, 4), activation='relu')(cnn_input)\n",
    "    x = layers.MaxPool2D(2, 2)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Conv2D(16, (4, 4), activation='relu')(x)\n",
    "    x = layers.MaxPool2D(2, 2)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Conv2D(16, (4, 4), activation='relu')(x)\n",
    "    x = layers.MaxPool2D(2, 2)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    cnn_output = layers.Flatten()(x)\n",
    "    \n",
    "    # LSTM Architecture\n",
    "    nlp_input =layers.Input(shape=(200,100))\n",
    "    y = layers.Masking()(nlp_input)\n",
    "    y = layers.BatchNormalization()(y)\n",
    "\n",
    "    y = layers.Bidirectional(layers.LSTM(32, activation='tanh', return_sequences=True))(y)\n",
    "    y = layers.Dropout(0.5)(y)\n",
    "\n",
    "    y = layers.Bidirectional(layers.LSTM(32, activation='tanh', return_sequences=True))(y)\n",
    "    y = layers.Dropout(0.5)(y)\n",
    "\n",
    "    y = layers.Bidirectional(layers.LSTM(32, activation='tanh', return_sequences=False))(y)\n",
    "    nlp_output = layers.Dropout(0.5)(y)\n",
    "    \n",
    "    # Define NLP model and concatenate output\n",
    "    combined = layers.concatenate([cnn_output, nlp_output])\n",
    "    \n",
    "    z = layers.Dense(64, activation='relu')(combined)\n",
    "    z = layers.Dropout(0.4)(z)\n",
    "    z = layers.Dense(32, activation='relu')(z)\n",
    "    z = layers.Dropout(0.2)(z)\n",
    "    \n",
    "    final_output = layers.Dense(1, activation='sigmoid')(z)\n",
    "\n",
    "    model = Model(inputs=[cnn_input, nlp_input],outputs=final_output)\n",
    "    \n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28f1235-7804-4904-b505-f304885a54a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfaaf7d-eb4a-4171-b9d9-f79f6e9a83aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1678cd2e-b921-47a7-96a5-5aec3a4d7183",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    (X_image, X_text),\n",
    "    y,\n",
    "    epochs=50,\n",
    "    batch_size=16,\n",
    "    validation_split=0.3,\n",
    "    callbacks=callbacks.EarlyStopping(patience=5, restore_best_weights=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be82d33-9c92-44fe-9d64-1d53bd052797",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m100",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m100"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
